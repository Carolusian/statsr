---
title: "House Prices - Advanced Regression Techniques"
output: 
  html_document:
    toc: true
    theme: united
    df_print: paged
---


```{r}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
library(pacman)
library(tidyverse)
library(tidymodels)
library(devtools)

# install_github("mkearney/kaggler")
# source("common.R")

tidymodels_prefer()
```

## Get the datasets

```{r}
train_df <- read_csv("../input/house-prices-advanced-regression-techniques/train.csv")
test_df <- read_csv("../input/house-prices-advanced-regression-techniques/test.csv")
sample_df <- read_csv("../input/house-prices-advanced-regression-techniques/sample_submission.csv")
```
## Let us explore the data

### Numeric predictors

- `SalePrice` shall be log transformed
- There is obviously linear correlation between `log(SalePrice)` and `log(GrLiveArea)` 
- There is a linear correlation between `log(SalePrice)` and `YearBuilt`
- There is a linear correlation between `log(SalePrice)` and `OverallQual`
- `XxxxSF` and `XxxxArea` variables: 
  - `TotalBsmtSF` and `1stFlrSF` is highly correlated with `Log(SalePrice)`
  - `BsmtUnfSF` and `2ndFlrSF` is also correlated
  - `GrLiveArea` and `2ndFlrSF` is also correlated
  - Others is relatively less correlated, can use PCA to reduce the dimensionality


```{r}
p_load(plotly)

train_df %>% 
  skimr::skim()

train_df %>% 
  select_if(is.numeric) %>% 
  corrr::correlate() %>%
  select(term, SalePrice) %>%
  arrange(desc(SalePrice)) %>%
  ggplot(aes(fct_reorder(term, SalePrice), SalePrice)) +
  geom_col() +
  coord_flip()

# Too many variables
train_df %>% 
  select_if(is.numeric) %>%
  corrr::correlate() %>% 
  corrr::rplot() + 
  scale_x_discrete(guide = guide_axis(angle = 90))

train_df %>%
  select_if(is.numeric) %>%
  select(ends_with("SF") | ends_with("Area")) %>%
  corrr::correlate() %>%
  corrr::rplot() +
  scale_x_discrete(guide = guide_axis(angle = 90))

# See if there is a strong correlation between "SF" variables and SalePrice
train_df %>%
  select_if(is.numeric) %>%
  select(Id, SalePrice, ends_with("SF")) %>%
  pivot_longer(cols = ends_with("SF")) %>%
  ggplot(aes(value, SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~ name, ncol = 2) +
  scale_y_log10(labels = scales::number)
  
train_df %>%
  ggplot(aes(YearBuilt, SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  scale_y_log10(labels = scales::number) 

# strong linear correlation
train_df %>%
  ggplot(aes(GrLivArea, SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) + 
  scale_y_log10(labels = scales::number) +
  scale_x_log10(labels = scales::number) 

train_df %>%
  ggplot(aes(LotArea, SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  scale_y_log10(labels = scales::number) +
  scale_x_log10(labels = scales::number) 

train_df %>%
  ggplot(aes(OverallQual, SalePrice)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  scale_y_log10(labels = scales::number) +
  scale_x_log10(labels = scales::number) 

train_df %>%
  ggplot(aes(GarageCars, GarageArea)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) 

# Use a 3d scatter plot to check relationship between 4 variables
train_df %>%
  plot_ly(x=~OverallQual, y=~GrLivArea, z=~log10(SalePrice), 
          type="scatter3d", color = ~YearBuilt) 

# Log scale worker better in this case as the original scale is right-skewed
# Log transformation avoids undue influence of expensive house prices
train_df %>%
  ggplot(aes(SalePrice)) +
  geom_histogram() +
  scale_x_log10(labels = scales::number)
```

### Categorical predictors

- `Neighborhood`
- `Condition2`
- `ExterQual`
- `BsmtQual`
- `GaragQual`
- `KitchenQual`
- `CentralAir`
- `Electrical`
- `Alley`

```{r}
train_df %>% 
  skimr::skim()

train_df %>%
  ggplot(aes(SalePrice, fct_reorder(Neighborhood, SalePrice))) +
  geom_boxplot() +
  scale_x_log10()

train_df %>%
  ggplot(aes(SalePrice, fill = Neighborhood)) +
  geom_density(alpha = .2) +
  scale_x_log10()

batch_boxplot <- function (df, col) {
  df %>% 
    ggplot(aes_string("SalePrice", fill = col)) +
    geom_density(alpha = .2) +
    scale_x_log10()
}

col_names <- train_df %>%
  select_if(is.character) %>%
  colnames() 

plots <- col_names %>%
  map(~ batch_boxplot(train_df, .))

plots
```


## Prepare data budget

```{r}
set.seed(123)

train_trans <- train_df %>%
  mutate(SalePrice = log10(SalePrice))
data_split <- initial_split(train_trans, prop = 0.80, strata = SalePrice)
train_data <- training(data_split)
heldout_data <- testing(data_split)
```

## Build a simple linear regression model with the predictors selected

```{r}
lm_rec <- recipe(SalePrice ~ 
         GrLivArea + YearBuilt + OverallQual + 
         TotalBsmtSF + `1stFlrSF` + `2ndFlrSF` + LowQualFinSF + BsmtUnfSF +
         Neighborhood + Condition2 + `ExterQual` + BsmtQual + GarageQual +
         KitchenQual + CentralAir,
       data = train_data) %>%
  step_log(GrLivArea, base = 10) %>%
  step_dummy(all_nominal_predictors())

lm_rec %>% prep() %>% juice() %>% skimr::skim()
lm_mod <- linear_reg()
lm_wf <- workflow(lm_rec, lm_mod)

folds <- vfold_cv(train_data, v = 5, strata = SalePrice)
ctrl <- control_resamples(save_pred = TRUE)
lm_rs <- fit_resamples(lm_wf, folds, control = ctrl)
lm_rs %>% collect_metrics()
lm_rs %>%
  collect_predictions() %>%
  ggplot(aes(SalePrice, .pred)) +
  geom_abline() +
  geom_point() +
  coord_obs_pred()

lm_rs %>%
  collect_predictions() %>%
  mutate(.resid = SalePrice - .pred) %>%
  ggplot(aes(.resid)) +
  geom_histogram()

lm_best <- select_best(lm_rs) 

lm_final <- finalize_workflow(lm_wf, lm_best)
lm_final %>%
  last_fit(data_split) %>%
  collect_metrics()
```

```{r}
# The prediction for low price house is higher, and lower for higher price houses
lm_rs %>%
  collect_predictions() %>%
  mutate(.resid = SalePrice - .pred) %>%
  ggplot(aes(SalePrice, .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm")
```


```{r}
# get RSME
lm_rs %>%
  collect_predictions(summarize = TRUE) %>%
  na.omit() %>%
  transmute(.pred = 10^.pred, SalePrice = 10^SalePrice, SqError = (.pred - SalePrice)^2) %>%
  pull(SqError) %>%
  mean() %>%
  sqrt()
```

## Check missing values

- It seems the missing values affects the performance of the linear model
- So need to have the missing values handled

- `LotFrontage` column is not used, just drop this column
- `GarageYrBlt` later than `YearBuilt`, can impute with `YearBuilt`
- `Alley` / `Street` are not useful
- `MasVnrType` impute with other
- `MasVnrArea` impute with 0
- `GarageXXX`: NA means no garage
- `BasementXXX`: NA means no Basement
- `Electrical`: can just drop the row
- `FireplaceQu` impute with other, NA means no fireplace
- `PoolQC`: NA means no pool

```{r}
skimr::skim(train_data)

# Alley / Street seems not useful
train_data %>% 
  count(Alley, Street) %>%
  ggplot(aes(Alley, n)) +
  geom_col()

# Impute missing GarageYrBlt with YearBuilt
train_data %>%
  count(GarageYrBlt, YearBuilt) %>%
  ggplot(aes(GarageYrBlt, YearBuilt)) +
  geom_jitter() 

train_data %>%
  count(MasVnrType)

train_data %>%
  filter(is.na(MasVnrArea)) %>%
  select(MasVnrArea, MasVnrType)
  
train_data %>%
  filter(is.na(GarageType)) 

train_data %>%
  count(Electrical, YearBuilt)

train_data %>%
  count(FireplaceQu)

train_data %>%
  filter(!is.na(PoolQC))

train_data %>%
  count(PoolQC, Fence)
```

- `LotFrontage` column is not used, just drop this column
- `GarageYrBlt` `NA` means Garage not exist, just drop this column
- `Alley` / `Street` are not useful
- `MasVnrType` impute with other
- `MasVnrArea` impute with 0
- `GarageXXX`: NA means no garage
- `BasementXXX`: NA means no Basement
- `Electrical`: can just drop the row
- `FireplaceQu` impute with other, NA means no fireplace
- `PoolQC`: NA means no pool

```{r}
# let us hanlde missing values here
handle_missing <- function (df) {
  df %>%
    select(-LotFrontage, -Alley, -Street, -GarageYrBlt, -Electrical, -MiscFeature, 
           -Fence, -Condition2, -RoofMatl, -Heating, -Exterior2nd, -ExterCond, -Functional, 
           -Utilities, -Exterior1st, -HeatingQC, -SaleType, -MSZoning, -BsmtFinSF1, -BsmtFinSF2, 
           -BsmtUnfSF, -TotalBsmtSF, -BsmtFullBath, 
           -BsmtHalfBath, -KitchenQual, -GarageCars, -GarageArea) %>%
    mutate(MasVnrType = if_else(is.na(MasVnrType), 'None', MasVnrType),
           MasVnrArea = if_else(is.na(MasVnrArea), 0, MasVnrArea)) %>% 
    mutate_if(is.character, replace_na, replace = 'None') %>%
    mutate_at(vars(contains("SalePrice")), ~ log10(.))
}
```

## Prepare data budget with missing value handled

```{r}
set.seed(2022)

train_clean <- handle_missing(train_df)
test_clean <- handle_missing(test_df)

data_split <- initial_split(train_clean, prop = 0.80, strata = SalePrice)
train_data <- training(data_split)
heldout_data <- testing(data_split)
```

```{r}
folds <- vfold_cv(train_clean, v = 5, strata = SalePrice)
```


## Try other models like KNN and Random Forest

linear regression have a good performance, but the residuals seems still a bit skrewed. Let us try some other models.

### KNN 

- knn seems not doing good 

```{r}
p_load(kknn)

knn_rec <- recipe(SalePrice ~ 
                    GrLivArea + YearBuilt + OverallQual,
                    data = train_data) %>%
  step_log(GrLivArea, base = 10) %>%
  step_normalize(all_numeric_predictors()) 
knn_mod <- nearest_neighbor(mode = "regression", neighbors = tune(), weight_func = tune())

knn_wf <- workflow() %>%
  add_recipe(knn_rec) %>%
  add_model(knn_mod)

knn_param <-
  knn_wf %>%
  parameters() %>%
  update(
    neighbors = neighbors(c(3, 50)),
    weight_func = weight_func(values = c("rectangular", "triangular", "epanechnikov", "biweight", "triweight", "cos", "inv"))
  )
knn_ctrl <- control_bayes(verbose = TRUE, save_pred = TRUE)
knn_search <- tune_bayes(knn_wf, folds, initial = 5, iter = 20, param_info = knn_param, control = knn_ctrl)

autoplot(knn_search, type = "performance", metric = "rmse")

collect_metrics(knn_search) %>% 
  dplyr::filter(.metric == "rmse") %>% 
  arrange(mean)

collect_predictions(knn_search) %>%
  mutate(.resid = .pred - SalePrice) %>%
  ggplot(aes(SalePrice, .resid)) +
  geom_point()
```
### Random Forest

```{r}
rf_rec <- recipe(SalePrice ~ ., data = train_data) %>%
  update_role(Id, new_role = "id variable")

rf_mod <- 
  rand_forest(mode = "regression", engine="ranger", 
              mtry = tune(), 
              trees = 1000, 
              min_n = tune())

rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_mod)

rf_ctrl <- control_grid(save_pred = TRUE)
rf_rs <- tune_grid(rf_wf, folds, grid = 20, control = rf_ctrl)

rf_rs %>%
  collect_metrics() %>%
  filter(.metric == 'rmse') %>%
  arrange(mean)

rf_rs %>%
  collect_predictions() %>%
  mutate(SqError = (.pred - SalePrice)^2) %>%
  pull(SqError) %>%
  mean() %>%
  sqrt()

rf_best <- rf_rs %>%
  select_best()
rf_best


last_fit <- 
  rf_wf %>%
  finalize_workflow(parameters = rf_best) %>%
  last_fit(data_split)

last_fit %>%
  collect_predictions() %>%
  ggplot(aes(SalePrice, .pred)) +
  geom_point() +
  coord_obs_pred()

rf_fit <- 
  rf_wf %>%
  finalize_workflow(parameters = rf_best) %>%
  fit(data = train_clean)
```
```{r}
p_load(vip)


rf_final <- finalize_model(rf_mod, rf_best)


rf_final %>%
  set_engine("ranger", importance = "permutation", model = ) %>%
  fit(SalePrice ~ .,
    data = prep(rf_rec) %>% juice() 
  ) %>%
  vip(geom = "point")
```


```{r}
rf_rec <- recipe(SalePrice ~ ., data = train_clean) %>%
  update_role(Id, new_role = "id variable")

rf_mod <- 
  rand_forest(mode = "regression",
              mtry = 8, 
              trees = 1000, 
              min_n = 9) %>%
  set_engine("ranger", importance = "permutation")

rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_mod)

rf_fit <- fit(rf_wf, data = train_clean)
bind_cols(
  test_clean %>% select(Id),   
  predict(rf_fit, new_data = test_clean) %>% transmute(SalePrice = 10^.pred)
) %>%
  write_csv("./submission.csv")
```

```{r}
rf_fit %>%
  extract_fit_parsnip() %>%
  vip()
```

